# LUYS.OS LAB â€” AntiBenchmark for High-Stakes AI

**LUYS.OS LAB** is an experimental *AntiBenchmark* module for evaluating model behavior under uncertainty.

Instead of classic benchmarks (MMLU, GSM8K, etc.) that measure only â€œknowledgeâ€ and accuracy, LAB focuses on a different question:

> **What does the model do when it does NOT have enough information?**

- Does it hallucinate a confident answer?
- Does it honestly say â€œI donâ€™t know, I need X, Y, Zâ€?
- Can it *stay silent* when the cost of a mistake is high?

LAB is designed as a **domain-agnostic framework** for high-stakes fields:
medicine, law, finance, engineering, journalism, etc.

---

## ğŸ” Core idea

LUYS AntiBenchmark (LAB) measures not only *what* the model says, but *how* it behaves:

- **Sultan Index (SI)** â€” â€œsultan indexâ€  
  How often the model gives confident answers when critical data is missing.

- **SLP / STR / JSR** â€” *Silence Protocol* and metrics of honest silence  
  How often the model correctly triggers â€œstop, I need more dataâ€.

- **TTS (Truth Traceability Score)** â€” traceability of facts  
  How many factual statements are backed by verifiable sources.

- **HRU (Hallucination Rate under Uncertainty)**  
  Hallucination rate specifically in under-specified / uncertain cases.

- **CVF (Cost per Validated Fact)** â€” unit economics of truth  
  How much one *validated* fact costs (compute + tools + human check).

- **REN / RGI** â€” resonance metrics (semantic / contextual resonance; defined separately).

Formal definitions and formulas live in:

- `docs/specs/antibenchmark.md`
- `docs/philosophy/resonance.md`

---

## ğŸ“ Repository structure

```text
luys-os-lab/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ antibenchmark/
â”‚       â”œâ”€â”€ __init__.py                  # exports LABEvaluator
â”‚       â”œâ”€â”€ evaluator.py                 # LAB evaluator (SI, STR/JSR, HRU, TTS, CVF, caps)
â”‚       â”œâ”€â”€ thresholds.toml              # per-domain thresholds
â”‚       â”œâ”€â”€ datasets/
â”‚       â”‚   â””â”€â”€ lab_core_50.json         # LAB-CORE-50 MVP dataset
â”‚       â””â”€â”€ ctm/
â”‚           â”œâ”€â”€ __init__.py
â”‚           â””â”€â”€ co_thinking.py           # Co-Thinking Mode (CTM) helper
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ specs/
â”‚   â”‚   â””â”€â”€ antibenchmark.md             # engineering spec for LAB
â”‚   â””â”€â”€ philosophy/
â”‚       â””â”€â”€ resonance.md                 # REN / resonance notes
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ run_lab_demo.py                  # single LAB run demo
â”‚   â”œâ”€â”€ run_lab_compare.py               # HONEST vs SULTAN demo
â”‚   â””â”€â”€ run_ctm_demo.py                  # CTM (Co-Thinking Mode) demo
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_antibenchmark.py            # smoke tests for metrics
â”‚
â”œâ”€â”€ pyproject.toml                       # Python package config
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

Local installation

Requires Python 3.9+.
git clone https://github.com/arutovan-droid/luys-os-lab.git
cd luys-os-lab

# (recommended) create virtual environment
python -m venv .venv

# Windows:
.\.venv\Scripts\activate
# macOS / Linux:
# source .venv/bin/activate

# install package in editable mode
pip install -e .

# install pytest for tests
pip install pytest

Running tests
pytest -q

You should see something like:

4 passed in 0.0xs

Minimal demo: running LABEvaluator

There is a small demo that runs LABEvaluator
against a dummy model implementation:

python examples/run_lab_demo.py

Expected output (shape, not exact numbers):
=== LAB RESULT ===
Domain: finance
Certification: PASS
Metrics: {'sultan_index': ..., 'jsr': ..., 'tts_critical': ..., 'hru': ..., 'cvf_impact': ..., 'rgi': ...}
Compliance: {'si_ok': True, 'jsr_ok': True, 'tts_ok': True, 'hru_ok': True, 'cvf_ok': True}

At this stage some metrics are still simplified by design â€”
the goal is to iterate toward full implementations with real models.

ğŸ­ Demo: HONEST vs SULTAN

LAB comes with a small opinionated demo to illustrate why behavior under uncertainty matters.

Run:
python examples/run_lab_compare.py

This script:

loads the LAB-CORE-50 dataset (finance cases by default),

simulates two extreme behaviors:

HONEST MODEL
â€“ triggers SLP when critical data is missing
â€“ always cites at least one source for critical claims

SULTAN MODEL
â€“ never triggers SLP
â€“ answers confidently without sources
â€“ does not mark speculation as hypothesis

Typical output:

=== HONEST MODEL ===
Certification : PASS
Sultan Index  : 0.000
HRU           : 0.000
TTS_critical  : 1.000
CVF_impact    : 0.000

=== SULTAN MODEL ===
Certification : FAIL
Sultan Index  : 1.000
HRU           : 1.000
TTS_critical  : 0.000
CVF_impact    : 0.000
Failed metrics:
 - Sultan Index 1.000 > 0.500 (hard cap)
 - HRU 1.000 > 0.500 (hard cap)

The idea is simple:

the HONEST model passes LAB because it asks for more data and cites sources;

the SULTAN model fails LAB because it hallucinates confidently under uncertainty.

LAB is designed to make this distinction explicit and measurable.

ğŸ¤ Co-Thinking Mode (CTM)

Besides classic AntiBenchmark metrics (Sultan Index, HRU, TTS, CVF),
luys-os-lab includes a lightweight Co-Thinking Mode (CTM) helper.

CTM turns a model from a â€œdo-it-for-meâ€ servant
into a â€œthink-with-meâ€ partner.

Core ideas:

The model should not jump straight to the final answer.

It must ask at least one clarifying question.

It must offer at least one alternative angle.

It must perform a short synthesis step together with the user.

We expose three simple CTM metrics:

CTI (Co-Thinking Index) â€“ how many sessions were completed without â€œcheatingâ€.

CDS (Clarification Depth Score) â€“ average number of meaningful clarifications per session.

CVR (Co-Thinking Velocity Ratio) â€“ how close the session length is to an â€œidealâ€ number of turns.

Run the demo:
python examples/run_ctm_demo.py

Example output:

=== CTM DEMO ===
Turns          : 5
Clarifications : 1
Synth steps    : 1
CTI            : 1.0
CDS            : 0.6
CVR            : 1.0

Using LAB in your own project

High-level usage example:

from core.antibenchmark import LABEvaluator

# 1. Create an evaluator for a specific domain
evaluator = LABEvaluator(domain="finance")

# 2. Implement your model with method: answer(case: dict) -> dict
class MyModel:
    def answer(self, case: dict) -> dict:
        # ... call your LLM / logic here ...
        return {
            "raw_answer": "...",
            "confidence": 0.73,        # float in [0, 1]
            "slp_triggered": False,    # did Silence Protocol trigger?
            "is_critical": True,       # is this a high-stakes answer?
            "sources": [
                "market://nyse/aapl/2024-11-26/close"
            ],
        }

model = MyModel()

# 3. Run LAB on a dataset
result = evaluator.evaluate(
    responses=[model.answer(c) for c in []],  # or integrate with real loop
    dataset=[]
)
# In real usage you would load LAB-CORE-50 or your own dataset.
AB does not lock you to any specific LLM provider:
you can plug in OpenAI, Gemini, local models, etc.
The only requirement is: answer(case) -> {raw_answer, confidence, slp_triggered, ...}.

ğŸ›£ï¸ Roadmap (draft)

âœ… Basic project structure (core/antibenchmark, docs, examples, tests)

âœ… LAB-CORE-50 dataset (10+ cases per domain: law, finance, medicine, engineering, journalism)

âœ… Editable Python package + smoke tests

âœ… HONEST vs SULTAN comparison demo

âœ… Co-Thinking Mode (CTM) helper and demo

Planned:

 Rich TTS implementation (Sourced / Derived / Unsourced analysis)

 REN / RGI integration into the evaluator

 LAB-EXT-5K extended dataset

 GitHub Actions: LAB as a CI/CD gatekeeper

 Publishing LAB spec as an open standard

ğŸ’¡ Philosophy

LUYS AntiBenchmark is not another leaderboard.

It is an attempt to define a trust standard:

Not â€œhow smart the model isâ€,
but how trustworthy it is when it does not know.

Contributions are welcome â€” cases, metrics, code, critique.
