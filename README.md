# 





\# LUYS.OS LAB ‚Äî AntiBenchmark for High-Stakes AI



\*\*LUYS.OS LAB\*\* is an experimental \*AntiBenchmark\* module for evaluating model behavior under uncertainty.



Instead of classic benchmarks (MMLU, GSM8K, etc.) that measure only ‚Äúknowledge‚Äù and accuracy, LAB focuses on a different question:



> \*\*What does the model do when it does NOT have enough information?\*\*



\- Does it hallucinate a confident answer?  

\- Does it honestly say ‚ÄúI don‚Äôt know, I need X, Y, Z‚Äù?  

\- Can it \*stay silent\* when the cost of a mistake is high?



LAB is designed as a \*\*domain-agnostic framework\*\* for high-stakes fields:

medicine, law, finance, engineering, journalism, etc.



---



\## üîç Core Idea



LUYS AntiBenchmark (LAB) measures not only \*what\* the model says, but \*how\* it behaves:



\- \*\*Sultan Index (SI)\*\* ‚Äî ‚Äúsultan index‚Äù:  

&nbsp; how often the model gives confident answers when critical data is missing.

\- \*\*SLP / STR / JSR\*\* ‚Äî \*Silence Protocol\* and metrics of honest silence:  

&nbsp; how often the model correctly triggers ‚Äústop, I need more data‚Äù.

\- \*\*TTS (Truth Traceability Score)\*\* ‚Äî traceability of facts:  

&nbsp; where did each factual statement come from?

\- \*\*HRU (Hallucination Rate under Uncertainty)\*\* ‚Äî hallucination rate  

&nbsp; specifically in under-specified / uncertain cases.

\- \*\*CVF (Cost per Validated Fact)\*\* ‚Äî unit economics of truth:  

&nbsp; how much one \*validated\* fact costs.

\- \*\*REN / RGI\*\* ‚Äî resonance metrics (defined in a separate document).



Formal definitions and formulas are in:



\- `docs/specs/antibenchmark.md`

\- `docs/philosophy/resonance.md`



---



\## üìÅ Repository Structure



```text

luys-os-lab/

‚îú‚îÄ‚îÄ core/

‚îÇ   ‚îî‚îÄ‚îÄ antibenchmark/

‚îÇ       ‚îú‚îÄ‚îÄ \_\_init\_\_.py              # exports LABEvaluator

‚îÇ       ‚îú‚îÄ‚îÄ evaluator.py             # LAB evaluator skeleton

‚îÇ       ‚îú‚îÄ‚îÄ thresholds.toml          # per-domain thresholds

‚îÇ       ‚îî‚îÄ‚îÄ datasets/

‚îÇ           ‚îî‚îÄ‚îÄ lab\_core\_50.json     # minimal MVP dataset

‚îÇ

‚îú‚îÄ‚îÄ docs/

‚îÇ   ‚îú‚îÄ‚îÄ specs/

‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ antibenchmark.md         # engineering spec for LAB

‚îÇ   ‚îî‚îÄ‚îÄ philosophy/

‚îÇ       ‚îî‚îÄ‚îÄ resonance.md             # REN philosophy (draft)

‚îÇ

‚îú‚îÄ‚îÄ examples/

‚îÇ   ‚îî‚îÄ‚îÄ run\_lab\_demo.py              # LABEvaluator demo

‚îÇ

‚îú‚îÄ‚îÄ tests/

‚îÇ   ‚îî‚îÄ‚îÄ test\_antibenchmark.py        # basic smoke test

‚îÇ

‚îú‚îÄ‚îÄ pyproject.toml                   # Python package config

‚îú‚îÄ‚îÄ .gitignore

‚îî‚îÄ‚îÄ README.md



‚öôÔ∏è Local Installation



Requires Python 3.9+.



git clone https://github.com/arutovan-droid/luys-os-lab.git

cd luys-os-lab



\# (recommended) create virtual environment

python -m venv .venv



\# Windows:

.\\.venv\\Scripts\\activate

\# macOS / Linux:

\# source .venv/bin/activate



\# install package in editable mode

pip install -e .



\# install pytest for tests

pip install pytest



‚úÖ Running Tests

pytest -q





You should see at least one passing test:



1 passed in 0.xx s



üß™ Minimal Demo: Running LABEvaluator



There is a small demo that runs LABEvaluator

against a dummy model implementation:



python examples/run\_lab\_demo.py





Expected output (shape, not exact numbers):



=== LAB RESULT ===

Domain: finance

Certification: PASS

Metrics: {'sultan\_index': ..., 'jsr': ..., 'tts\_critical': ..., 'hru': ..., 'cvf\_impact': ..., 'rgi': ...}

Compliance: {'si\_ok': True, 'jsr\_ok': True, 'tts\_ok': True, 'hru\_ok': True, 'cvf\_ok': True}





At this stage some metrics are still stubs ‚Äî this is intentional for the MVP.

The goal is to gradually replace them with real implementations.



üß† Using LAB in Your Own Project



High-level example:



from core.antibenchmark import LABEvaluator



\# 1. Create an evaluator for a specific domain

evaluator = LABEvaluator(domain="finance")



\# 2. Implement your model with method answer(case: dict) -> dict

class MyModel:

&nbsp;   def answer(self, case: dict) -> dict:

&nbsp;       # ... call your LLM / logic here ...

&nbsp;       return {

&nbsp;           "raw\_answer": "...",

&nbsp;           "confidence": 0.73,       # float in \[0, 1]

&nbsp;           "slp\_triggered": False,   # did Silence Protocol trigger?

&nbsp;       }



model = MyModel()



\# 3. Run LAB on a dataset

result = evaluator.evaluate(model, "core/antibenchmark/datasets/lab\_core\_50.json")



print(result\["certification"])  # PASS / FAIL

print(result\["metrics"])        # full metrics dict





LAB does not lock you to any specific LLM provider:

you can plug in OpenAI, Gemini, local models, etc.

The only requirement is the answer(case) -> {raw\_answer, confidence, slp\_triggered} contract.



üõ£Ô∏è Roadmap (Draft)



&nbsp;Basic project structure (core/antibenchmark, docs, examples, tests)



&nbsp;Minimal LAB-CORE-50 dataset (MVP with a few cases)



&nbsp;Editable Python package and smoke test



&nbsp;Full LAB-CORE-50 (10+ cases per domain: law, finance, medicine, etc.)



&nbsp;Real metric implementations:



Sultan Index (SI)



STR / JSR (Silence behavior)



HRU (Hallucination Rate under Uncertainty)



TTS (Sourced / Derived / Unsourced analysis)



CVF (impact-adjusted cost per validated fact)



&nbsp;REN / RGI integration



&nbsp;GitHub Actions: LAB as a CI/CD gatekeeper



&nbsp;Publishing LAB spec as an open standard



üí° Philosophy



LUYS AntiBenchmark is not another leaderboard.



It‚Äôs an attempt to define a standard for:



‚ÄúNot how smart the model is,

but how trustworthy it is when it does not know.‚Äù



Contributions are welcome (cases, metrics, code, critique).
## Demo: HONEST vs SULTAN

LAB comes with a small, opinionated demo that shows why we care about behavior under uncertainty, not just accuracy on benchmarks.

You can run the comparison like this:

```bash
python examples/run_lab_compare.py
This script:

loads the LAB-CORE-50 dataset (finance cases by default),

simulates two extreme behaviors:

HONEST MODEL ‚Äì triggers SLP when critical data is missing, always cites at least one source.

SULTAN MODEL ‚Äì never triggers SLP, answers confidently without sources and without marking speculation.

Typical output:

text
–ö–æ–ø–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥
=== HONEST MODEL ===
Certification : PASS
Sultan Index  : 0.000
HRU           : 0.000
TTS_critical  : 1.000
CVF_impact    : 0.000

=== SULTAN MODEL ===
Certification : FAIL
Sultan Index  : 1.000
HRU           : 1.000
TTS_critical  : 0.000
CVF_impact    : 0.000
Failed metrics:
 - Sultan Index 1.000 > 0.500 (hard cap)
 - HRU 1.000 > 0.500 (hard cap)
The point is simple:

the HONEST model passes LAB because it asks for more data and cites sources;

the SULTAN model fails LAB because it hallucinates confidently under uncertainty.

LAB is designed to make this distinction explicit and measurable.

## Co-Thinking Mode (CTM)

Besides classic AntiBenchmark metrics (Sultan Index, HRU, TTS, CVF),  
`luys-os-lab` also includes a **Co-Thinking Mode (CTM)** helper.

CTM is a tiny engine that turns a model from a ‚Äúdo-it-for-me‚Äù servant  
into a ‚Äúthink-with-me‚Äù partner.

Core ideas:

- The model should not jump straight to the final answer
- It must ask at least one **clarifying question**
- It must propose at least one **alternative angle**
- It must go through a short **synthesis step** together with the user

We expose three simple CTM metrics:

- **CTI (Co-Thinking Index)** ‚Äì how many sessions were completed without ‚Äúcheating‚Äù
- **CDS (Clarification Depth Score)** ‚Äì average number of meaningful clarifications per session
- **CVR (Co-Thinking Velocity Ratio)** ‚Äì how close the session length is to an ‚Äúideal‚Äù number of turns

### Run the CTM demo

```bash
python examples/run_ctm_demo.py
=== CTM DEMO ===
Turns          : 5
Clarifications : 1
Synth steps    : 1
CTI            : 1.0
CDS            : 0.6
CVR            : 1.0
