# 





\# LUYS.OS LAB ‚Äî AntiBenchmark for High-Stakes AI



\*\*LUYS.OS LAB\*\* is an experimental \*AntiBenchmark\* module for evaluating model behavior under uncertainty.



Instead of classic benchmarks (MMLU, GSM8K, etc.) that measure only ‚Äúknowledge‚Äù and accuracy, LAB focuses on a different question:



> \*\*What does the model do when it does NOT have enough information?\*\*



\- Does it hallucinate a confident answer?  

\- Does it honestly say ‚ÄúI don‚Äôt know, I need X, Y, Z‚Äù?  

\- Can it \*stay silent\* when the cost of a mistake is high?



LAB is designed as a \*\*domain-agnostic framework\*\* for high-stakes fields:

medicine, law, finance, engineering, journalism, etc.



---



\## üîç Core Idea



LUYS AntiBenchmark (LAB) measures not only \*what\* the model says, but \*how\* it behaves:



\- \*\*Sultan Index (SI)\*\* ‚Äî ‚Äúsultan index‚Äù:  

&nbsp; how often the model gives confident answers when critical data is missing.

\- \*\*SLP / STR / JSR\*\* ‚Äî \*Silence Protocol\* and metrics of honest silence:  

&nbsp; how often the model correctly triggers ‚Äústop, I need more data‚Äù.

\- \*\*TTS (Truth Traceability Score)\*\* ‚Äî traceability of facts:  

&nbsp; where did each factual statement come from?

\- \*\*HRU (Hallucination Rate under Uncertainty)\*\* ‚Äî hallucination rate  

&nbsp; specifically in under-specified / uncertain cases.

\- \*\*CVF (Cost per Validated Fact)\*\* ‚Äî unit economics of truth:  

&nbsp; how much one \*validated\* fact costs.

\- \*\*REN / RGI\*\* ‚Äî resonance metrics (defined in a separate document).



Formal definitions and formulas are in:



\- `docs/specs/antibenchmark.md`

\- `docs/philosophy/resonance.md`



---



\## üìÅ Repository Structure



```text

luys-os-lab/

‚îú‚îÄ‚îÄ core/

‚îÇ   ‚îî‚îÄ‚îÄ antibenchmark/

‚îÇ       ‚îú‚îÄ‚îÄ \_\_init\_\_.py              # exports LABEvaluator

‚îÇ       ‚îú‚îÄ‚îÄ evaluator.py             # LAB evaluator skeleton

‚îÇ       ‚îú‚îÄ‚îÄ thresholds.toml          # per-domain thresholds

‚îÇ       ‚îî‚îÄ‚îÄ datasets/

‚îÇ           ‚îî‚îÄ‚îÄ lab\_core\_50.json     # minimal MVP dataset

‚îÇ

‚îú‚îÄ‚îÄ docs/

‚îÇ   ‚îú‚îÄ‚îÄ specs/

‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ antibenchmark.md         # engineering spec for LAB

‚îÇ   ‚îî‚îÄ‚îÄ philosophy/

‚îÇ       ‚îî‚îÄ‚îÄ resonance.md             # REN philosophy (draft)

‚îÇ

‚îú‚îÄ‚îÄ examples/

‚îÇ   ‚îî‚îÄ‚îÄ run\_lab\_demo.py              # LABEvaluator demo

‚îÇ

‚îú‚îÄ‚îÄ tests/

‚îÇ   ‚îî‚îÄ‚îÄ test\_antibenchmark.py        # basic smoke test

‚îÇ

‚îú‚îÄ‚îÄ pyproject.toml                   # Python package config

‚îú‚îÄ‚îÄ .gitignore

‚îî‚îÄ‚îÄ README.md



‚öôÔ∏è Local Installation



Requires Python 3.9+.



git clone https://github.com/arutovan-droid/luys-os-lab.git

cd luys-os-lab



\# (recommended) create virtual environment

python -m venv .venv



\# Windows:

.\\.venv\\Scripts\\activate

\# macOS / Linux:

\# source .venv/bin/activate



\# install package in editable mode

pip install -e .



\# install pytest for tests

pip install pytest



‚úÖ Running Tests

pytest -q





You should see at least one passing test:



1 passed in 0.xx s



üß™ Minimal Demo: Running LABEvaluator



There is a small demo that runs LABEvaluator

against a dummy model implementation:



python examples/run\_lab\_demo.py





Expected output (shape, not exact numbers):



=== LAB RESULT ===

Domain: finance

Certification: PASS

Metrics: {'sultan\_index': ..., 'jsr': ..., 'tts\_critical': ..., 'hru': ..., 'cvf\_impact': ..., 'rgi': ...}

Compliance: {'si\_ok': True, 'jsr\_ok': True, 'tts\_ok': True, 'hru\_ok': True, 'cvf\_ok': True}





At this stage some metrics are still stubs ‚Äî this is intentional for the MVP.

The goal is to gradually replace them with real implementations.



üß† Using LAB in Your Own Project



High-level example:



from core.antibenchmark import LABEvaluator



\# 1. Create an evaluator for a specific domain

evaluator = LABEvaluator(domain="finance")



\# 2. Implement your model with method answer(case: dict) -> dict

class MyModel:

&nbsp;   def answer(self, case: dict) -> dict:

&nbsp;       # ... call your LLM / logic here ...

&nbsp;       return {

&nbsp;           "raw\_answer": "...",

&nbsp;           "confidence": 0.73,       # float in \[0, 1]

&nbsp;           "slp\_triggered": False,   # did Silence Protocol trigger?

&nbsp;       }



model = MyModel()



\# 3. Run LAB on a dataset

result = evaluator.evaluate(model, "core/antibenchmark/datasets/lab\_core\_50.json")



print(result\["certification"])  # PASS / FAIL

print(result\["metrics"])        # full metrics dict





LAB does not lock you to any specific LLM provider:

you can plug in OpenAI, Gemini, local models, etc.

The only requirement is the answer(case) -> {raw\_answer, confidence, slp\_triggered} contract.



üõ£Ô∏è Roadmap (Draft)



&nbsp;Basic project structure (core/antibenchmark, docs, examples, tests)



&nbsp;Minimal LAB-CORE-50 dataset (MVP with a few cases)



&nbsp;Editable Python package and smoke test



&nbsp;Full LAB-CORE-50 (10+ cases per domain: law, finance, medicine, etc.)



&nbsp;Real metric implementations:



Sultan Index (SI)



STR / JSR (Silence behavior)



HRU (Hallucination Rate under Uncertainty)



TTS (Sourced / Derived / Unsourced analysis)



CVF (impact-adjusted cost per validated fact)



&nbsp;REN / RGI integration



&nbsp;GitHub Actions: LAB as a CI/CD gatekeeper



&nbsp;Publishing LAB spec as an open standard



üí° Philosophy



LUYS AntiBenchmark is not another leaderboard.



It‚Äôs an attempt to define a standard for:



‚ÄúNot how smart the model is,

but how trustworthy it is when it does not know.‚Äù



Contributions are welcome (cases, metrics, code, critique).

